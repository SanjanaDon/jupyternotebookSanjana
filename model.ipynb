{"cells":[{"cell_type":"markdown","id":"085da90a","metadata":{},"source":["<img src=\"https://blobcity.com/assets/svg/logos/logo.svg\" alt=\"AI in the Cloud\" width=\"150\" height=\"50\">"]},{"cell_type":"markdown","id":"c42d7bd0","metadata":{},"source":["# Classification Problem\r\n"]},{"cell_type":"code","execution_count":null,"id":"8884fe11","metadata":{},"outputs":[],"source":["# imports\r","import numpy as np\r","import pandas as pd\r","import matplotlib.pyplot as plt\r","import seaborn as se\r","import warnings\r","from sklearn.model_selection import train_test_split\r","from sklearn.preprocessing import LabelEncoder\r","from sklearn.metrics import classification_report,plot_confusion_matrix\r","warnings.filterwarnings('ignore')\r\n","from sklearn.preprocessing import MinMaxScaler\r","from sklearn.feature_selection import SelectKBest,f_regression,f_classif\r","from sklearn.preprocessing import StandardScaler\r","from sklearn.preprocessing import PolynomialFeatures\r","from sklearn.svm import SVC\r\n"]},{"cell_type":"markdown","id":"8213a531","metadata":{},"source":["### Data Fetch\n"," Pandas is an open-source, BSD-licensed library providing high-performance,easy-to-use data manipulation and data analysis tools."]},{"cell_type":"code","execution_count":null,"id":"6e11a687","metadata":{},"outputs":[],"source":["# Data Fetch\r","file=''\r","df=pd.read_csv(file)\r","df.head()\r\n"]},{"cell_type":"markdown","id":"04e1e9f5","metadata":{},"source":["### Feature Selection\n"," It is the process of reducing the number of input variables when developing a predictive model.Used to reduce the number of input variables to reduce the computational cost of modelling and,in some cases,to improve the performance of the model."]},{"cell_type":"code","execution_count":null,"id":"a0006991","metadata":{},"outputs":[],"source":["# Selected Columns\r","features=['LIST OF FEATURES/COLUMN NAMES']\r","target='TARGET COLUMN NAME'\r\n","# X & Y\r","X=df[features]\r","Y=df[target]\r\n"]},{"cell_type":"markdown","id":"142bacdf","metadata":{},"source":["### Data Encoding\n"," Converting the string classes data in the datasets by encoding them to integer either using OneHotEncoding or LabelEncoding"]},{"cell_type":"code","execution_count":null,"id":"ef74cd36","metadata":{},"outputs":[],"source":["# Handling AlphaNumeric Features\r","X=pd.get_dummies(X)\r\n"]},{"cell_type":"markdown","id":"27d1177b","metadata":{},"source":["### Correlation Matrix\n"," In order to check the correlation between the features, we will plot a correlation matrix. It is effective in summarizing a large amount of data where the goal is to see patterns."]},{"cell_type":"code","execution_count":null,"id":"6d63756f","metadata":{},"outputs":[],"source":["f,ax = plt.subplots(figsize=(18, 18))\r","matrix = np.triu(X.corr())\r","se.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, mask=matrix)\r","plt.show()\n"]},{"cell_type":"markdown","id":"8778ec54","metadata":{},"source":["### Multi-colinearity Test\n"," Dropping Highly Correlated Features to due similar features distributions\n"]},{"cell_type":"code","execution_count":null,"id":"0c89a6ad","metadata":{},"outputs":[],"source":["def dropHighCorrelationFeatures(X):\n","        cor_matrix = X.corr()\n","        upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n","        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n","        if to_drop!=[]: return X.drop(to_drop, axis=1)\n","        else: return X\n","X=dropHighCorrelationFeatures(X)\r","X.head()"]},{"cell_type":"markdown","id":"25b7ac0b","metadata":{},"source":["### Best Feature Selection\n"," selecting 'n' best feature on the basis of ANOVA or Univariate Linear Regression Test. where ANOVA is used for Classification problem and Univariate Linear Regression for Regression problems\r"]},{"cell_type":"code","execution_count":null,"id":"2ec4373e","metadata":{},"outputs":[],"source":["\n","def get_feature_importance(X,Y,score_func):\n","    fit = SelectKBest(score_func=score_func, k=X.shape[1]).fit(X,Y)\n","    dfscores,dfcolumns = pd.DataFrame(fit.scores_),pd.DataFrame(X.columns)\n","    df = pd.concat([dfcolumns,dfscores],axis=1)\n","    df.columns = ['features','Score'] \n","    df['Score']=MinMaxScaler().fit_transform(np.array(df['Score']).reshape(-1,1))\n","    result=dict(df.values)\n","    val=dict(sorted(result.items(), key=lambda item: item[1],reverse=False))\n","    keylist=[]\n","    for key, value in val.items():\n","        if value < 0.01: keylist.append(key)\n","    X=X.drop(keylist,axis=1)\n","    plt.figure(figsize = (12, 6))\n","    plt.barh(range(len(val)), list(val.values()), align='center')\n","    plt.yticks(range(len(val)),list(val.keys()))\n","    plt.xlabel(\"Importance\")\n","    plt.ylabel(\"Feature\")\n","    plt.tight_layout()\n","    plt.show()\n","    return X\n","X=get_feature_importance(X,Y,score_func=f_classif)\n","        "]},{"cell_type":"markdown","id":"962780bd","metadata":{},"source":["### Data Rescaling\n"," Feature scaling or Data scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization"]},{"cell_type":"code","execution_count":null,"id":"2eaf36b9","metadata":{},"outputs":[],"source":["columns=X.columns\r","X=StandardScaler().fit_transform(X)\r","X=pd.DataFrame(data = X,columns = columns)\r","X.head()\r"]},{"cell_type":"markdown","id":"b07163bd","metadata":{},"source":["### Train & Test\n"," The train-test split is a procedure for evaluating the performance of an algorithm.The procedure involves taking a dataset and dividing it into two subsets.The first subset is utilized to fit/train the model.The second subset is used for prediction.The main motive is to estimate the performance of the model on new data."]},{"cell_type":"code","execution_count":null,"id":"1d2bb381","metadata":{},"outputs":[],"source":["# Data split for training and testing\r","X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=123)\r\n"]},{"cell_type":"markdown","id":"0b92450d","metadata":{},"source":["### Feature Transformation\n","  Feature transformation is a mathematical transformation in which we apply a mathematical formula to data and transform the values which are useful for our further analysis."]},{"cell_type":"code","execution_count":null,"id":"055d21fa","metadata":{},"outputs":[],"source":["polynomialfeatures=PolynomialFeatures()\r","X_train=polynomialfeatures.fit_transform(X_train)\r","X_test=polynomialfeatures.transform(X_test)\r"]},{"cell_type":"markdown","id":"083fb6ff","metadata":{},"source":["### Model\n","Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n","\n","A Support Vector Machine is a discriminative classifier formally defined by a separating hyperplane. In other terms, for a given known/labelled data points, the SVM outputs an appropriate hyperplane that classifies the inputted new cases based on the hyperplane. In 2-Dimensional space, this hyperplane is a line separating a plane into two segments where each class or group occupied on either side.\n","\n","Here we have used SVC, the svc implementation is based on libsvm.  \n","\n","### Model Tuning Parameters\n","1. C -> Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","\n","2. kernel -> Specifies the kernel type to be used in the algorithm.\n","\n","3. gamma -> Gamma is a hyperparameter that we have to set before the training model. Gamma decides how much curvature we want in a decision boundary.\n","\n","4. degree -> Degree of the polynomial kernel function ('poly'). Increasing degree parameter leads to higher training times.\n"]},{"cell_type":"code","execution_count":null,"id":"f14d9859","metadata":{},"outputs":[],"source":["# Model Initialization\r","model=SVC()\r","model.fit(X_train,Y_train)\r\n"]},{"cell_type":"markdown","id":"3f8b815e","metadata":{},"source":["### Accuracy Metrics\n"," Performance metrics are a part of every machine learning pipeline. They tell you if you're making progress, and put a number on it. All machine learning models,whether it's linear regression, or a SOTA technique like BERT, need a metric to judge performance."]},{"cell_type":"code","execution_count":null,"id":"1d82575e","metadata":{},"outputs":[],"source":["# Confusion Matrix\r","plot_confusion_matrix(model,X_test,Y_test,cmap=plt.cm.Blues)\r\n","# Classification Report\r","print(classification_report(Y_test,model.predict(X_test)))\r\n"]}],"metadata":{},"nbformat":4,"nbformat_minor":5}